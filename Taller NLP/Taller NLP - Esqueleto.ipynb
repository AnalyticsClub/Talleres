{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El procesamiento del lenguaje natural (PLN o NLP) es un campo dentro de la inteligencia artificial y la lingüística aplicada que estudia las interacciones mediante uso del lenguaje natural entre los seres humanos y las máquinas. Más concretamente se centra en el procesamiento de las comunicaciones humanas, dividiéndolas en partes, e identificando los elementos más relevantes del mensaje. Con la Comprensión y Generación de Lenguaje Natural, busca que las máquinas consigan entender, interpretar y manipular el lenguaje humano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"NLP.jpg\", width=\"450\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de los casos e uso de NLP se encuentran:\n",
    "* Sentiment Analysis\n",
    "* Named Entity Recognition (NER)\n",
    "* Automatic Text Summarization\n",
    "* Automated Question Answering \n",
    "* Content categorization\n",
    "* Speech to Text and Text to Speech\n",
    "\n",
    "Mas especificamente podemos ver:\n",
    "* Las aplicaciones de traducción como Google Translate utilizan NLP para la traducción.\n",
    "* NLP ayuda en la detección de noticias falsas. Por ejemplo, el grupo NLP del MIT ha desarrollado un sistema que identifica si una fuente está políticamente sesgada o no. Basándose en la precisión sugiere si confiar o no en una fuente de noticias.\n",
    "* NLP ayuda en la clasificación de correos electrónicos. Empresas como Google y Yahoo analizan el texto de nuestros correos electrónicos para filtrar y detener los spams.\n",
    "* NLP puede ayudar a reconocer y predecir la condición médica o enfermedad del paciente sobre la base de su propio habla y algunos otros registros. Por ejemplo, Amazon utiliza un servicio llamado Amazon Comprehend Medical. Utiliza NLP para entender el estado de la enfermedad del paciente, medicamentos y resultados del tratamiento de las notas de voz del paciente, informes de ensayos clínicos, etc.\n",
    "* Los Traders utilizan NLP para rastrear noticias, informes y comentarios. Todas las perspectivas obtenidas se alimentan en un algoritmo de trading para generar los máximos beneficios.\n",
    "* Varias aplicaciones de procesamiento de textos como Microsoft Word, Grammarly, etc. utilizan NLP para comprobar los errores gramaticales en el texto.\n",
    "* Las organizaciones utilizan NLP para realizar análisis de opiniones sobre los datos de clientes recopilados de las redes sociales y otros recursos. Esto les ayuda a obtener información sobre las opciones y opiniones de los clientes sobre sus productos.\n",
    "* Siri, Cortana, Alexa y otros asistentes de voz personales hacen uso de NLP para responder a nuestros comandos vocales. \n",
    "(Techvidvan, 2021)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen varias librerias especializadas en NLP en python, las mas utilizadas son Spacy y Nltk. Ambas poseen una variedad de herramientas para la limpieza, procesamiento y manejo. Hoy vamos a trabajar un pco con ambas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "<td> <img src=\"SPACY.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"NLTK.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para introducir las herramientas básicas de NLP y darles un punto de partida vamos a trabajar con un problema clasico: Analisis de sentimiento en twitter. Para esto, contamos con el dataset <a href=\"https://www.kaggle.com/kazanova/sentiment140\">*Sentiment140*</a> el cual contiene 1.6M de tweet con su respectiva polaridad: 0 indicando un sentimiento negativo y 4 indicando un sentimiento positivo.\n",
    "\n",
    "Debido al tamaño de la base, vamos a trabajar con una muestra de 10000 tweets unicamente.\n",
    "\n",
    "Revisemos la base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos unas graficas para explorar los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets Positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets Negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expresiones Regulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de un texto como lo es un tweet, nos encontramos con varios elementos que no nos aportan nada para determinar su sentimiento, como: las menciones, las URL o los hashtags. En este sentido, lo mas logico seria eliminarlos, pero cómo hacemos esto si cada uno de estos elementos puede tener infinidad de formas? Para este problema, las expresiones regulares llegan al rescate.\n",
    "\n",
    "* Las expresiones regulares son patrones que se utilizan para hacer coincidir combinaciones de caracteres en cadenas. <a href=\"https://docs.python.org/3/library/re.html\">(Librería Re Python)</a>\n",
    "\n",
    "En nuestro caso, no sabemos como lucen los usuarios o los hashtags, pero sabemos que todos comparten un formato particular (usuarios empiezan con un @ o hastags con un #) con esto podemos construir una expresión regular que encuentre todo texto que siga dicho patrón y lo elimine de nuestro texto. \n",
    "\n",
    "Veamos un poco como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\S hace match con cualquier caracter que no sea un espacio.\n",
    "* '+' hace match con una o mas repeticiones del patrón especificado.\n",
    "\n",
    "En este sentido:\n",
    "* La primera expresión hace match con cualquier cosa que este precedida de un @ (Elimina menciones)\n",
    "* La segunda y tercer expresión hacen match con cualquier cosa que este precedida de un http o www. (Elimina links y URLs)\n",
    "* La cuarta expresión hace match con cualquier cosa que este precedida de un # (Elimina hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ademas de realizar una limpieza debido a la naturaleza de nuestro texto (tweets), antes de trabajar con cualquier tipo de texto se debe hacer un procesamiento del mismo debido al simple hecho de ser texto. Normalmente se siguen unos pasos \"estandar\".\n",
    "\n",
    "* Dentro de cualquier idioma existen palabras que se repiten demasiado y no aportan ningun tipo de información, estas palabras se conocen como **stopwords**. En español estas serian palabras como: a, en, el, la, entre otras.\n",
    "* Numeros, simbolos y signos de puntuacion no nos aportan información relevante (en este caso), deberíamos eliminarlos.\n",
    "* Palabras como jugar, juega, jugó todas indican la misma accion y tienen el mismo proposito. En este sentido, deberiamos asegurarnos de que las tratemos como iguales, para hacerlo aplicamos procedimientos conocidos como **stemming** y **lemmatization**. El objetivo de estas tecnicas es reducir las palabras a su forma base o a su raíz.\n",
    "* Dentro del texto puede existir la misma palabra escrita con mayúscula y con minúscula, es recomendable transformar todo a minúscula como base antes de trabajar.\n",
    "* Es posible que nuestro texto tenga emojis, tambien deberiamos tratarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creemos una funcion que nos ayude a preprocesar el texto, aplicando todas estas reglas que definimos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos la función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con nuestro texto limpio podemos empezar a trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización del Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos que los modelos de ML solo trabajan con números entonces, que hacemos con texto? \n",
    "\n",
    "Para poder trabajar con texto debemos hacerle una transformación, de letras a números, para que el computador y el modelo puedan entenderlo y trabajar con él. \n",
    "\n",
    "Cómo hacemos esto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay muchas maneras de vectorizar el texto, en el taller veremos las tres mas populares:\n",
    "* Bag of Words\n",
    "* TF-IDF\n",
    "* Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un procedimiento de extracción de características muy común para oraciones y documentos es el enfoque de bolsa de palabras (BOW). En este enfoque, observamos el histograma de las palabras dentro del texto, es decir, consideramos que cada palabra es una variable diferente. (Página 69, Métodos de redes neuronales en el procesamiento del lenguaje natural, 2017.)\n",
    "\n",
    "Es un método para convertir texto en vectores dispersos. Solo tiene en cuenta si las palabras están (o no) presentes en una oración determinada. Es muy simple, pero también la forma más ingenua de vectorización de texto.\n",
    "\n",
    "**Ejemplo**: Consideremos dos oraciones\n",
    "* Hello, how are you? (**S1**)\n",
    "* Hello, how is it going? (**S2**)\n",
    "\n",
    "Acá los vectores generados por *Bag of words* son\n",
    "\n",
    " | | Hello | how   | are   | you   | is | it | going |\n",
    "|:---:|:-------------:|:-----------:|:------:|:------:|:-----------:|:------:|:------:|\n",
    "| S1 | 1  | 1       | 1   | 1    | 0 | 0| 0|\n",
    "| S2 | 1 | 1    | 0   | 0    | 1 | 1| 1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF es una ponderación que se utiliza a menudo en la recuperación de información y la minería de texto. Este peso es una medida estadística que se utiliza para evaluar la importancia de una palabra para un documento en una colección o corpus. La importancia aumenta proporcionalmente al número de veces que aparece una palabra en el documento, pero se compensa con la frecuencia de la palabra en el corpus. (tfidf.com)\n",
    "\n",
    "TF-IDF se presenta como una mejora a BOW en el sentido que tiene en cuenta no solo si una palabra en particular está presente en una oración, sino que también pondera cada palabra de acuerdo con su frecuencia e importancia general.\n",
    "\n",
    "El coeficiente TF-IDF se compone de dos partes:\n",
    "\n",
    "$$TF(t) = \\frac{Número \\textrm{ } de \\textrm{ } veces \\textrm{ } que \\textrm{ } aparece \\textrm{ } el  \\textrm{ } término \\textrm{ } t \\textrm{ } en \\textrm{ } un \\textrm{ } documento} {Número \\textrm{ } total \\textrm{ } de \\textrm{ } términos \\textrm{ } en \\textrm{ } el \\textrm{ } documento}$$\n",
    "\n",
    "\n",
    " $$IDF(t) = log \\left(\\frac{Número\\textrm{ } total\\textrm{ } de\\textrm{ } documentos}{Número\\textrm{ } de\\textrm{ } documentos\\textrm{ } con\\textrm{ } el\\textrm{ } término\\textrm{ } t}\\right)$$\n",
    " \n",
    " \n",
    " $$ \\rm{TF-IDF(t)} = TF(t) * IDF(t) $$\n",
    " \n",
    "**Ejemplo**:\n",
    "\n",
    "Consideremos un documento que contiene 100 palabras, en el que la palabra gato aparece 3 veces. El TF para gato es entonces:\n",
    "\n",
    "$$TF_{gato} = \\frac{3}{100} = 0.03$$\n",
    "\n",
    "Ahora, suponga que tenemos 10 millones de documentos y la palabra gato aparece en mil de ellos. Entonces, el IDF se calcula como: \n",
    "\n",
    "$$IDF_{gato} = log\\left(\\frac{10000000}{1000}\\right) = 4$$\n",
    "\n",
    "De esta manera, el peso TF-IDF es el producto de estas dos cantidades: \n",
    "\n",
    "$$\\rm{TF-IDF}_{gato} = TF_{gato} * IDF_{gato} =  0.03 * 4 = 0.12$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddigs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Word Embeddings* son el método más popular para representar texto. Son capaces de generar una representación que mantiene relaciones semánticas y sintácticas entre palabras.\n",
    "\n",
    "<div>\n",
    "<img src=\"WE.png\", width=\"450\"/>\n",
    "</div>\n",
    "\n",
    "En términos simples, son una representación vectorial de la palabra / texto que captura la esencia del lenguaje.\n",
    "\n",
    "Los métodos más populares son:\n",
    "* Word2Vec\n",
    "* GloVe\n",
    "* ElMo\n",
    "* BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo *Word2Vec* consiste en una red neuronal poco profunda que se utiliza para encontrar representaciones vectoriales de las palabras dentro de un texto. \n",
    "\n",
    "Existen ds formas de construir el modelo:\n",
    "* En la primera, la red neuronal recibe como entrada un vector binario (*One Hot Encoded*) de la palabra e intenta predecir las palabras que la rodean (*Skip-gram*) \n",
    "* En la segunda, la red recibe varios vectores binarios de las palabras de contexto e intenta predecir un vector objetivo (*CBOW*)\n",
    "\n",
    "<div>\n",
    "<img src=\"WV_.PNG\", width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Las representaciones vectoriales de las palabras son el resultado de entrenar los pesos de la capa oculta de una red neuronal poco profunda.\n",
    "\n",
    "<div>\n",
    "<img src=\"WV2_.PNG\", width=\"750\"/>\n",
    "</div>\n",
    "\n",
    "Y estos vectores poseen unas propiedades muy interesantes: \n",
    "\n",
    "* Los *Word Embeddings* resultantes no solo transforman el texto en números, sino que también capturan el significado semántico de las palabras. \n",
    "* Si dos palabras diferentes son similares en contexto, la salida de la RN debería ser similar. Para que la salida sea similar, la fila correspondiente de la matriz de peso de la capa oculta debe ser similar.\n",
    "* Las palabras que tienen un significado contextual similar aparecerán más cerca en el espacio vectorial.\n",
    "* !Se pueden realizar operaciones entre los vectores y sus resultados tienen sentido!\n",
    "\n",
    "<div>\n",
    "<img src=\"WV3_.png\", width=\"700\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenemos nuestro propio modelo Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que nuestro modelo esta entrenado, hagamos una prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya teniendo el modelo podemos crear vectores para representar cada tweet y usarlos para entrenar un modelo de clasificación como en los casos anteriores. Para eso creemos un par de funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la funcion tokenize separamos el texto en palabras las cuales vectorizamos y sumamos para encontrar el vector del tweet. Utilizamos este vector como feature de nuestro modelo de detección de sentimiento.\n",
    "\n",
    "**Nota**: A pesar de estar utilizando un modelo Word2Vec es posible que el clasificador nos de peor que en los casos anteriores, esto es debido a que no tenemos un corpus lo suficientemente grande para aprender las relaciones semanticas y sintacticas del lenguaje. En otras palabras al tener tan poco texto, los vectores no son my buenos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> “If I have seen further than others, it is by standing upon the shoulders of giants.” - Isaac Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Transfer Learning* es un método de aprendizaje automático en el que un modelo desarrollado para una tarea se reutiliza como punto de partida para un modelo en una segunda tarea.\n",
    "\n",
    "Es un enfoque popular en el aprendizaje profundo donde los modelos pre-entrenados se utilizan como el punto de partida en la visión por computadora y las tareas de procesamiento del lenguaje natural dados los vastos recursos informáticos y de tiempo necesarios para desarrollar modelos de red neuronal sobre estos problemas y a partir de los enormes saltos en habilidad que proporcionan en los problemas relacionados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning con los vectores pre-entrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qué es una Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una red neuronal es un modelo simplificado que emula el modo en que el cerebro humano procesa la información: Funciona simultaneando un número elevado de unidades de procesamiento interconectadas que parecen versiones abstractas de neuronas.\n",
    "\n",
    "Las unidades de procesamiento se organizan en capas. Hay tres partes normalmente en una red neuronal : una capa de entrada, con unidades que representan los campos de entrada; una o varias capas ocultas; y una capa de salida, con una unidad o unidades que representa el campo o los campos de destino. Las unidades se conectan con fuerzas de conexión variables (o ponderaciones). Los datos de entrada se presentan en la primera capa, y los valores se propagan desde cada neurona hasta cada neurona de la capa siguiente. al final, se envía un resultado desde la capa de salida.\n",
    "\n",
    "<div>\n",
    "<img src=\"NN.jpg\", width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "La red aprende examinando los registros individuales, generando una predicción para cada registro y realizando ajustes a las ponderaciones cuando realiza una predicción incorrecta. Este proceso se repite muchas veces y la red sigue mejorando sus predicciones hasta haber alcanzado uno o varios criterios de parada.\n",
    "\n",
    "Al principio, todas las ponderaciones son aleatorias y las respuestas que resultan de la red son, posiblemente, disparatadas. La red aprende a través del entrenamiento. Continuamente se presentan a la red ejemplos para los que se conoce el resultado, y las respuestas que proporciona se comparan con los resultados conocidos. La información procedente de esta comparación se pasa hacia atrás a través de la red, cambiando las ponderaciones gradualmente. A medida que progresa el entrenamiento, la red se va haciendo cada vez más precisa en la replicación de resultados conocidos. Una vez entrenada, la red se puede aplicar a casos futuros en los que se desconoce el resultado. (IBM, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redes Neuronales Recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los humanos no empiezan a pensar desde cero cada segundo. Al leer estas palabras, comprendes cada palabra basándote en tu comprensión de las palabras anteriores. No tiras todo a la borda y empiezas a pensar desde cero otra vez. Tus pensamientos tienen persistencia.\n",
    "\n",
    "Las redes neuronales tradicionales no pueden hacer esto, y parece una deficiencia importante. Por ejemplo, imagine que desea clasificar qué tipo de evento está sucediendo en cada punto de una película. No está claro cómo una red neuronal tradicional podría usar su razonamiento sobre eventos anteriores en la película para informar a los posteriores.\n",
    "\n",
    "Las redes neuronales recurrentes abordan este problema. Son redes con bucles en ellos, lo que permite que la información persista. (Understanding LSTM Networks, 2015)\n",
    "\n",
    "<div>\n",
    "<img src=\"RNN-unrolled.png\", width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Esta naturaleza en cadena revela que las redes neuronales recurrentes están íntimamente relacionadas con secuencias y listas. Son la arquitectura natural de la red neuronal para usar para estos datos.\n",
    "\n",
    "¡Y ciertamente se usan! En los últimos años, ha habido un éxito increíble aplicando RNNs a una variedad de problemas: reconocimiento de voz, modelado de idiomas, traducción, subtitulado de imágenes... La lista continúa\n",
    "\n",
    "Esencial para estos éxitos es el uso de *LSTMs*, un tipo muy especial de red neuronal recurrente que funciona, para muchas tareas, mucho mejor que la versión estándar. Casi todos los resultados emocionantes basados en redes neuronales recurrentes se logran con ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las LSTMs están diseñadas explícitamente para evitar el problema de dependencia a largo plazo. Recordar información durante largos períodos de tiempo es prácticamente su comportamiento predeterminado, no algo que les cuesta aprender!\n",
    "\n",
    "Todas las redes neuronales recurrentes tienen la forma de una cadena de módulos repetidos de red neuronal. En los RNN estándar, este módulo de repetición tendrá una estructura muy simple, como una sola capa de tanh.\n",
    "\n",
    "<div>\n",
    "<img src=\"RNN.png\", width=\"350\"/>\n",
    "</div>\n",
    "\n",
    "Los LSTMs también tienen esta estructura similar a la cadena, pero el módulo repetidor tiene una estructura diferente. En lugar de tener una sola capa de red neuronal, hay cuatro, interactuando de una manera muy especial.\n",
    "\n",
    "<div>\n",
    "<img src=\"LSTM.png\", width=\"350\"/>\n",
    "</div>\n",
    "\n",
    "La clave de las LSTMs es el estado de celda, la línea horizontal que se ejecuta a través de la parte superior del diagrama.\n",
    "\n",
    "El estado celular es como una cinta transportadora. Se ejecuta directamente por toda la cadena, con sólo algunas interacciones lineales menores. Es muy fácil para la información fluir a lo largo de ella sin cambios.\n",
    "\n",
    "El LSTM tiene la capacidad de quitar o agregar información al estado de la célula, cuidadosamente regulado por estructuras llamadas puertas. Las puertas son una manera de dejar pasar la información opcionalmente. Se componen de una capa de red neuronal sigmoide y una operación de multiplicación puntual.\n",
    "\n",
    "La capa sigmoide genera números entre cero y uno, describiendo cuánto de cada componente debe ser dejado pasar. Un valor de cero significa \"no deje pasar nada\", mientras que un valor de uno significa \"¡deja pasar todo!\". Un LSTM tiene tres de estas puertas, para proteger y controlar el estado celular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: Esto es solo un abrebocas de redes neuronales recurrentes, si quieren entender mas a fondo acerca de ellas y de las LSTM, pueden visitar el increible blog de donde salio esta información <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/n\">Colah's Blog</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al entrenar unicamente por 10 epochs, ya obtenemos mejores resutados que en cualquiera de los modelos/metodologias anteriores! Ese es el poder de las redes neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
